{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-class_classification_of_handwritten_digits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joardar-aditya/Assignment-5/blob/joardar-aditya/multi_class_classification_of_handwritten_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPa95uXvcpcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifying Handwritten Digits with Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "Fdpn8b90u8Tp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST.png)"
      ]
    },
    {
      "metadata": {
        "id": "c7HLCm66Cs2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "  * Train both a linear model and a neural network to classify handwritten digits from the classic [MNIST](http://yann.lecun.com/exdb/mnist/) data set\n",
        "  * Compare the performance of the linear and neural network classification models\n",
        "  * Visualize the weights of a neural-network hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "HSEh-gNdu8T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to map each input image to the correct numeric digit. We will create a NN with a few hidden layers and a Softmax layer at the top to select the winning class."
      ]
    },
    {
      "metadata": {
        "id": "2NMdE1b-7UIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, let's download the data set, import TensorFlow and other utilities, and load the data into a *pandas* `DataFrame`. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
      ]
    },
    {
      "metadata": {
        "id": "4LJ4SD8BWHeh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "7959881e-725e-402c-b364-0931bc22b187"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "mnist_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "# Use just the first 10,000 records for training/validation.\n",
        "mnist_dataframe = mnist_dataframe.head(10000)\n",
        "\n",
        "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "mnist_dataframe.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6849</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3824</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1958</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4523</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
              "6849    3    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "562     6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "3824    9    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "1958    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "4523    3    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "\n",
              "      778  779  780  781  782  783  784  \n",
              "6849    0    0    0    0    0    0    0  \n",
              "562     0    0    0    0    0    0    0  \n",
              "3824    0    0    0    0    0    0    0  \n",
              "1958    0    0    0    0    0    0    0  \n",
              "4523    0    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "kg0-25p2mOi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each row represents one labeled example. Column 0 represents the label that a human rater has assigned for one handwritten digit. For example, if Column 0 contains '6', then a human rater interpreted the handwritten character as the digit '6'.  The ten digits 0-9 are each represented, with a unique class label for each possible digit. Thus, this is a multi-class classification problem with 10 classes."
      ]
    },
    {
      "metadata": {
        "id": "PQ7vuOwRCsZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST-Matrix.png)"
      ]
    },
    {
      "metadata": {
        "id": "dghlqJPIu8UM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Columns 1 through 784 contain the feature values, one per pixel for the 28×28=784 pixel values. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent shades of gray. Most of the pixel values are 0; you may want to take a minute to confirm that they aren't all 0.  For example, adjust the following text block to print out the values in column 72."
      ]
    },
    {
      "metadata": {
        "id": "2ZkrL5MCqiJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "e2788e98-fe58-44e8-e6ed-609fb99b6014"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.loc[:, 72:72]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>72</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6849</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3824</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1958</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4523</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4517</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2893</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3727</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8838</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      72\n",
              "6849   0\n",
              "562    0\n",
              "3824   0\n",
              "1958   0\n",
              "4523   0\n",
              "...   ..\n",
              "4517   0\n",
              "2893   0\n",
              "3727   0\n",
              "1209   0\n",
              "8838   0\n",
              "\n",
              "[10000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "vLNg2VxqhUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's parse out the labels and features and look at a few examples. Note the use of `loc` which allows us to pull out columns based on original location, since we don't have a header row in this data set."
      ]
    },
    {
      "metadata": {
        "id": "JfFWWvMWDFrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_labels_and_features(dataset):\n",
        "  \"\"\"Extracts labels and features.\n",
        "  \n",
        "  This is a good place to scale or transform the features if needed.\n",
        "  \n",
        "  Args:\n",
        "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
        "      monochrome pixel values on the remaining columns, in row major order.\n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      labels: A Pandas `Series`.\n",
        "      features: A Pandas `DataFrame`.\n",
        "  \"\"\"\n",
        "  labels = dataset[0]\n",
        "\n",
        "  # DataFrame.loc index ranges are inclusive at both ends.\n",
        "  features = dataset.loc[:,1:784]\n",
        "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
        "  features = features / 255\n",
        "\n",
        "  return labels, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFY_-7vZu8UU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "be81a869-0655-47b9-b0eb-2f58e8a1b9e4"
      },
      "cell_type": "code",
      "source": [
        "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
        "training_examples.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.3    0.2    1.0    0.2    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 7500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "4-Vgg-1zu8Ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7f9c63ce-bdfc-4b0f-cf22-59ca1fb6e41a"
      },
      "cell_type": "code",
      "source": [
        "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
        "validation_examples.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.8    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 2500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "wrnAI1v6u8Uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show a random example and its corresponding label."
      ]
    },
    {
      "metadata": {
        "id": "s-euVJVtu8Ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "18467f58-e6e1-4882-fb2a-5a451b6b8947"
      },
      "cell_type": "code",
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFXCAYAAAAro2x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBACD/wNvgpd0JMGtvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ScmYX7xdZMXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Build a Linear Model for MNIST\n",
        "\n",
        "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
        "\n",
        "You'll notice that in addition to reporting accuracy, and plotting Log Loss over time, we also display a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were misclassified as other classes. Which digits get confused for each other?\n",
        "\n",
        "Also note that we track the model's error using the `log_loss` function. This should not be confused with the loss function internal to `LinearClassifier` that is used for training."
      ]
    },
    {
      "metadata": {
        "id": "cpoVC4TSdw5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  \n",
        "  # There are 784 pixels in each image.\n",
        "  return set([tf.feature_column.numeric_column('pixels', shape=784)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMmL89yGeTfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we'll make separate input functions for training and for prediction. We'll nest them in `create_training_input_fn()` and `create_predict_input_fn()`, respectively, so we can invoke these functions to return the corresponding `_input_fn`s to pass to our `.train()` and `.predict()` calls."
      ]
    },
    {
      "metadata": {
        "id": "OeS47Bmn5Ms2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
        "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
        "\n",
        "  Args:\n",
        "    features: The training features.\n",
        "    labels: The training labels.\n",
        "    batch_size: Batch size to use during training.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns batches of training features and labels during\n",
        "    training.\n",
        "  \"\"\"\n",
        "  def _input_fn(num_epochs=None, shuffle=True):\n",
        "    # Input pipelines are reset with each call to .train(). To ensure model\n",
        "    # gets a good sampling of data, even when number of steps is small, we \n",
        "    # shuffle all the data before creating the Dataset object\n",
        "    idx = np.random.permutation(features.index)\n",
        "    raw_features = {\"pixels\":features.reindex(idx)}\n",
        "    raw_targets = np.array(labels[idx])\n",
        "   \n",
        "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zoGWAoohrwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_predict_input_fn(features, labels, batch_size):\n",
        "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
        "\n",
        "  Args:\n",
        "    features: The features to base predictions on.\n",
        "    labels: The labels of the prediction examples.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns features and labels for predictions.\n",
        "  \"\"\"\n",
        "  def _input_fn():\n",
        "    raw_features = {\"pixels\": features.values}\n",
        "    raw_targets = np.array(labels)\n",
        "    \n",
        "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "        \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6DjSLZMu8Um",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, and a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `LinearClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create a LinearClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(),\n",
        "      n_classes=10,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItHIUyv2u8Ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Spend 5 minutes seeing how well you can do on accuracy with a linear model of this form. For this exercise, limit yourself to experimenting with the hyperparameters for batch size, learning rate and steps.**\n",
        "\n",
        "Stop if you get anything above about 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "yaiIhIQqu8Uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1121
        },
        "outputId": "f874ef65-4856-4e93-d856-aded315e671d"
      },
      "cell_type": "code",
      "source": [
        "classifier = train_linear_classification_model(\n",
        "             learning_rate=0.02,\n",
        "             steps=1000,\n",
        "             batch_size=10,\n",
        "             training_examples=training_examples,\n",
        "             training_targets=training_targets,\n",
        "             validation_examples=validation_examples,\n",
        "             validation_targets=validation_targets)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Training model...\n",
            "LogLoss error (on validation data):\n",
            "  period 00 : 16.08\n",
            "  period 01 : 12.14\n",
            "  period 02 : 8.51\n",
            "  period 03 : 8.39\n",
            "  period 04 : 8.29\n",
            "  period 05 : 6.89\n",
            "  period 06 : 7.34\n",
            "  period 07 : 6.24\n",
            "  period 08 : 6.74\n",
            "  period 09 : 6.12\n",
            "Model training finished.\n",
            "Final accuracy (on validation data): 0.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEH5CQlsIIYTwE/8HZPjHSEVHGV4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAFnCAYAAACM3c9QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBARkSQMEiIikoRBQkREkjBIiIhIkv8FI5LY+OAS+YQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "266KQvZoMxMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "metadata": {
        "id": "lRWcn24DM3qa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a set of parameters that should attain roughly 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "TGlBMrUoM1K_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = train_linear_classification_model(\n",
        "    learning_rate=0.03,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mk095OfpPdOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Replace the Linear Classifier with a Neural Network\n",
        "\n",
        "**Replace the LinearClassifier above with a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and find a parameter combination that gives 0.95 or better accuracy.**\n",
        "\n",
        "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
      ]
    },
    {
      "metadata": {
        "id": "rm8P_Ttwu8U4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Replace the linear classifier with a neural network.\n",
        "#\n",
        "def train_nn_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns,\n",
        "      n_classes=10,\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOfmiSvqu8U9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have a good model, double check that you didn't overfit the validation set by evaluating on the test data that we'll load below.\n"
      ]
    },
    {
      "metadata": {
        "id": "evlB5ubzu8VJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDuLd2Hcu8VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Calculate accuracy on the test set.\n",
        "#\n",
        "classifier = train_nn_classification_model(\n",
        "    learning_rate=0.03,\n",
        "    steps=1000,\n",
        "    batch_size=50,\n",
        "    hidden_units=[100, 100],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sfw3LH0Oycm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for a possible solution."
      ]
    },
    {
      "metadata": {
        "id": "XatDGFKEO374",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below is almost identical to the original `LinearClassifer` training code, with the exception of the NN-specific configuration, such as the hyperparameter for hidden units."
      ]
    },
    {
      "metadata": {
        "id": "kdNTx8jkPQUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nn_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns,\n",
        "      n_classes=10,\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfzsTYGPPU8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = train_nn_classification_model(\n",
        "    learning_rate=0.05,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    hidden_units=[100, 100],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXvrOgtUR-zD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we verify the accuracy on the test set."
      ]
    },
    {
      "metadata": {
        "id": "scQNpDePSFjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVaWpWKvSHmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_test_input_fn = create_predict_input_fn(\n",
        "    test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX2mQBAEcisO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3: Visualize the weights of the first hidden layer.\n",
        "\n",
        "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
        "\n",
        "The input layer of our model has `784` weights corresponding to the `28×28` pixel input images. The first hidden layer will have `784×N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28×28` images by *reshaping* each of the `N` `1×784` arrays of weights into `N` arrays of size `28×28`.\n",
        "\n",
        "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
      ]
    },
    {
      "metadata": {
        "id": "eUC0Z8nbafgG",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classifier.get_variable_names())\n",
        "\n",
        "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
        "\n",
        "print(\"weights0 shape:\", weights0.shape)\n",
        "\n",
        "num_nodes = weights0.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
        "for coef, ax in zip(weights0.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kL8MEhNgrx9N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
        "\n",
        "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
        "\n",
        "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
        "\n",
        "What differences do you see visually for the different levels of convergence?"
      ]
    }
  ]
}